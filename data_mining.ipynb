{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f3b8ca-3845-4f53-a080-9a2c295bf413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import folium\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e9298-5e58-40fd-b1ad-19f2ed7dbaf7",
   "metadata": {},
   "source": [
    "# Load and clean data\n",
    "\n",
    "## Step 1: Load and Clean Data\n",
    "We start by loading the NYC crash data from a CSV file. This dataset contains detailed information about traffic crashes, including their locations, contributing factors, and injury counts. Data cleaning involves removing duplicates, handling missing values, and ensuring all location data is valid and usable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca7e870-265c-479e-b71a-c107b18f305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "data = pd.read_csv('crash_data.csv', low_memory=False)\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1342184-9871-47a4-b3c8-15473af3963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning...\n",
      "Data cleaning completed. Number of entries remaining: 1893744\n"
     ]
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    This function cleans the input data by performing several steps:\n",
    "    - Removes duplicate entries to ensure unique crash records.\n",
    "    - Handles missing numerical values by replacing them with zero, ensuring no crash is excluded due to incomplete data.\n",
    "    - Filters out invalid or missing latitude and longitude values, retaining only meaningful geographic information.\n",
    "    \"\"\"\n",
    "    print(\"Starting data cleaning...\")\n",
    "    df = df.drop_duplicates()\n",
    "    num_cols = ['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
    "                'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
    "                'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
    "                'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED']\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "    df = df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "    df = df[(df['LATITUDE'] != 0) & (df['LONGITUDE'] != 0) & (df['LONGITUDE'] > -180) & (df['LONGITUDE'] < 180)]\n",
    "    df['CRASH DATETIME'] = pd.to_datetime(df['CRASH DATE'] + ' ' + df['CRASH TIME'], errors='coerce')\n",
    "    df = df.drop(columns=['CRASH DATE', 'CRASH TIME'])\n",
    "    factor_cols = ['CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', \n",
    "                   'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4', \n",
    "                   'CONTRIBUTING FACTOR VEHICLE 5']\n",
    "    for col in factor_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 'Unknown'\n",
    "        else:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    print(f\"Data cleaning completed. Number of entries remaining: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "cleaned_data = clean_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2254655-4fc6-4a0d-b095-0bacb41ca3ee",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Step 2: Classification\n",
    "In this step, we predict whether a crash involves injuries using a machine learning model. The Gradient Boosting Classifier is used due to its robustness in handling imbalanced datasets. This process includes feature selection, scaling, and balancing the data to improve prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808e7016-f00a-483d-9860-1c2e73fc1499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting classification...\n",
      "Selecting best features...\n",
      "Scaling features...\n",
      "Applying SMOTE for oversampling...\n",
      "Splitting data for classification...\n",
      "Training GradientBoostingClassifier with class weighting...\n",
      "Classification completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68    433551\n",
      "           1       0.69      0.52      0.59    433365\n",
      "\n",
      "    accuracy                           0.64    866916\n",
      "   macro avg       0.65      0.64      0.64    866916\n",
      "weighted avg       0.65      0.64      0.64    866916\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify_crashes(df):\n",
    "    \"\"\"\n",
    "    This function handles the classification process:\n",
    "    - Converts categorical contributing factor features into numerical representations using one-hot encoding.\n",
    "    - Selects the top 20 features using the SelectKBest method to focus on the most relevant data.\n",
    "    - Balances the dataset with SMOTE to handle class imbalance, ensuring fair training for the classifier.\n",
    "    - Trains a Gradient Boosting Classifier and evaluates its performance using a classification report.\n",
    "    \"\"\"\n",
    "    print(\"Starting classification...\")\n",
    "    # Convert contributing factors to numerical values (one-hot encoding)\n",
    "    df_encoded = pd.get_dummies(df, columns=[col for col in df.columns if col.startswith('CONTRIBUTING FACTOR')])\n",
    "    \n",
    "    # Define features AFTER encoding\n",
    "    features = ['LATITUDE', 'LONGITUDE'] + [col for col in df_encoded.columns if 'CONTRIBUTING FACTOR' in col]\n",
    "    \n",
    "    # Ensure all features exist in the encoded dataframe\n",
    "    X = df_encoded[features]\n",
    "    y = (df['NUMBER OF PERSONS INJURED'] > 0).astype(int)  # Binary target variable\n",
    "\n",
    "    # Feature selection\n",
    "    print(\"Selecting best features...\")\n",
    "    selector = SelectKBest(f_classif, k=20)  # Updated to f_classif\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "    # Scale features\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    \n",
    "    # Apply SMOTE to balance the classes\n",
    "    print(\"Applying SMOTE for oversampling...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "    \n",
    "    # Split data\n",
    "    print(\"Splitting data for classification...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train a GradientBoostingClassifier with class weighting\n",
    "    print(\"Training GradientBoostingClassifier with class weighting...\")\n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(f\"Classification completed.\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "classify_crashes(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54012485-bd07-42b0-a6c9-237f3b9cf612",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "## Step 3: Clustering\n",
    "This step focuses on grouping crashes into clusters based on geographic and severity attributes. Clustering helps identify crash hotspots, aiding in targeted safety measures. The KMeans algorithm is used to create clusters, with severity scores normalized for effective grouping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e0d9e5c-1ab0-4c6d-beb9-e9e2560b600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting clustering for crash severity hotspots...\n",
      "Fitting KMeans with 35 clusters...\n",
      "Clustering completed. Cluster centers:\n",
      "Cluster 0: Location (40.8103, -73.9304), Severity Score: -0.40\n",
      "Cluster 1: Location (40.6577, -73.9674), Severity Score: 1.97\n",
      "Cluster 2: Location (40.7409, -73.9600), Severity Score: 0.78\n",
      "Cluster 3: Location (40.7208, -73.9050), Severity Score: 5.52\n",
      "Cluster 4: Location (40.7037, -73.9270), Severity Score: 17.04\n",
      "Cluster 5: Location (40.7737, -73.8532), Severity Score: 3.15\n",
      "Cluster 6: Location (40.7234, -73.9041), Severity Score: 4.34\n",
      "Cluster 7: Location (40.6483, -73.9279), Severity Score: -0.40\n",
      "Cluster 8: Location (40.6652, -32.7685), Severity Score: -0.40\n",
      "Cluster 9: Location (40.7220, -73.9093), Severity Score: 10.67\n",
      "Cluster 10: Location (40.7227, -73.9261), Severity Score: -0.99\n",
      "Cluster 11: Location (40.7419, -73.7620), Severity Score: 37.51\n",
      "Cluster 12: Location (40.7124, -73.9040), Severity Score: 7.89\n",
      "Cluster 13: Location (40.7490, -73.8493), Severity Score: -0.40\n",
      "Cluster 14: Location (40.6941, -73.9548), Severity Score: 26.45\n",
      "Cluster 15: Location (40.6950, -73.7673), Severity Score: 0.78\n",
      "Cluster 16: Location (40.5936, -74.1321), Severity Score: -0.40\n",
      "Cluster 17: Location (40.6945, -73.9664), Severity Score: -0.40\n",
      "Cluster 18: Location (40.7276, -73.9039), Severity Score: 6.71\n",
      "Cluster 19: Location (40.5921, -74.1314), Severity Score: 0.78\n",
      "Cluster 20: Location (40.7189, -73.8553), Severity Score: 0.78\n",
      "Cluster 21: Location (40.6580, -73.9656), Severity Score: 3.15\n",
      "Cluster 22: Location (40.7625, -73.9000), Severity Score: 13.37\n",
      "Cluster 23: Location (40.7697, -73.8524), Severity Score: 1.97\n",
      "Cluster 24: Location (40.6951, -73.7718), Severity Score: -0.40\n",
      "Cluster 25: Location (40.8571, -73.8750), Severity Score: -0.40\n",
      "Cluster 26: Location (40.7287, -73.9126), Severity Score: 9.08\n",
      "Cluster 27: Location (40.8455, -73.8911), Severity Score: 0.78\n",
      "Cluster 28: Location (40.6864, -73.8817), Severity Score: -0.40\n",
      "Cluster 29: Location (40.6449, -73.9469), Severity Score: 0.78\n",
      "Cluster 30: Location (40.7491, -73.9749), Severity Score: -0.40\n",
      "Cluster 31: Location (40.6174, -73.9861), Severity Score: -0.40\n",
      "Cluster 32: Location (40.6790, -73.9385), Severity Score: 49.35\n",
      "Cluster 33: Location (40.7211, -73.9346), Severity Score: 0.19\n",
      "Cluster 34: Location (40.7475, -73.9236), Severity Score: 20.71\n",
      "Calculating cluster radii...\n",
      "Cluster 0 radius: 200.00 meters\n",
      "Cluster 1 radius: 200.00 meters\n",
      "Cluster 2 radius: 200.00 meters\n",
      "Cluster 3 radius: 200.00 meters\n",
      "Cluster 4 radius: 200.00 meters\n",
      "Cluster 5 radius: 200.00 meters\n",
      "Cluster 6 radius: 200.00 meters\n",
      "Cluster 7 radius: 200.00 meters\n",
      "Cluster 8 radius: 0.00 meters\n",
      "Cluster 9 radius: 200.00 meters\n",
      "Cluster 10 radius: 200.00 meters\n",
      "Cluster 11 radius: 200.00 meters\n",
      "Cluster 12 radius: 200.00 meters\n",
      "Cluster 13 radius: 200.00 meters\n",
      "Cluster 14 radius: 200.00 meters\n",
      "Cluster 15 radius: 200.00 meters\n",
      "Cluster 16 radius: 200.00 meters\n",
      "Cluster 17 radius: 200.00 meters\n",
      "Cluster 18 radius: 200.00 meters\n",
      "Cluster 19 radius: 200.00 meters\n",
      "Cluster 20 radius: 200.00 meters\n",
      "Cluster 21 radius: 200.00 meters\n",
      "Cluster 22 radius: 200.00 meters\n",
      "Cluster 23 radius: 200.00 meters\n",
      "Cluster 24 radius: 200.00 meters\n",
      "Cluster 25 radius: 200.00 meters\n",
      "Cluster 26 radius: 200.00 meters\n",
      "Cluster 27 radius: 200.00 meters\n",
      "Cluster 28 radius: 200.00 meters\n",
      "Cluster 29 radius: 200.00 meters\n",
      "Cluster 30 radius: 200.00 meters\n",
      "Cluster 31 radius: 200.00 meters\n",
      "Cluster 32 radius: 0.00 meters\n",
      "Cluster 33 radius: 200.00 meters\n",
      "Cluster 34 radius: 200.00 meters\n",
      "Cluster assignment completed.\n",
      "Clustered Data Sample:\n",
      "           LATITUDE  LONGITUDE  SEVERITY_SCORE  Cluster\n",
      "16653996  40.682265 -73.845133             0.0     28.0\n",
      "23334923  40.718430 -74.000534             2.0     30.0\n",
      "1199569   40.764435 -73.722690             0.0     24.0\n",
      "15242286  40.666620 -73.835464             0.0     28.0\n",
      "13252929  40.761234 -73.963890             0.0     30.0\n",
      "Cluster Details:\n",
      "{'Cluster': 0, 'Latitude': 40.81025977105539, 'Longitude': -73.93044709581793, 'Severity_Score': -0.4000963844637264, 'Radius_meters': 200}\n",
      "{'Cluster': 1, 'Latitude': 40.65769961008978, 'Longitude': -73.96737177380291, 'Severity_Score': 1.9689248123056424, 'Radius_meters': 200}\n",
      "{'Cluster': 2, 'Latitude': 40.74088573987426, 'Longitude': -73.95996150604877, 'Severity_Score': 0.7847279534623471, 'Radius_meters': 200}\n",
      "{'Cluster': 3, 'Latitude': 40.72076494971335, 'Longitude': -73.90497265839475, 'Severity_Score': 5.522614867166003, 'Radius_meters': 200}\n",
      "{'Cluster': 4, 'Latitude': 40.703671162068964, 'Longitude': -73.92700316206897, 'Severity_Score': 17.04126709447347, 'Radius_meters': 200}\n",
      "{'Cluster': 5, 'Latitude': 40.773701363353744, 'Longitude': -73.85319918947245, 'Severity_Score': 3.152693431339613, 'Radius_meters': 200}\n",
      "{'Cluster': 6, 'Latitude': 40.723426542545205, 'Longitude': -73.9040644756056, 'Severity_Score': 4.338072616840075, 'Radius_meters': 200}\n",
      "{'Cluster': 7, 'Latitude': 40.64827294042079, 'Longitude': -73.92787614950265, 'Severity_Score': -0.4000963844637254, 'Radius_meters': 200}\n",
      "{'Cluster': 8, 'Latitude': 40.665226, 'Longitude': -32.768513, 'Severity_Score': -0.40009638446370493, 'Radius_meters': 0.0}\n",
      "{'Cluster': 9, 'Latitude': 40.7219926048, 'Longitude': -73.909250888, 'Severity_Score': 10.668266402581915, 'Radius_meters': 200}\n",
      "{'Cluster': 10, 'Latitude': 40.72271106313883, 'Longitude': -73.92606055403085, 'Severity_Score': -0.9923675096267071, 'Radius_meters': 200}\n",
      "{'Cluster': 11, 'Latitude': 40.74186925, 'Longitude': -73.76195394999999, 'Severity_Score': 37.50525562596649, 'Radius_meters': 200}\n",
      "{'Cluster': 12, 'Latitude': 40.71242873534483, 'Longitude': -73.90404948793103, 'Severity_Score': 7.891699367817903, 'Radius_meters': 200}\n",
      "{'Cluster': 13, 'Latitude': 40.7490424971183, 'Longitude': -73.84929421206671, 'Severity_Score': -0.400096384463725, 'Radius_meters': 200}\n",
      "{'Cluster': 14, 'Latitude': 40.694148455555556, 'Longitude': -73.95482083333333, 'Severity_Score': 26.44952795625769, 'Radius_meters': 200}\n",
      "{'Cluster': 15, 'Latitude': 40.69500020386003, 'Longitude': -73.76725576463642, 'Severity_Score': 0.784640159287672, 'Radius_meters': 200}\n",
      "{'Cluster': 16, 'Latitude': 40.59364609417218, 'Longitude': -74.13210544584467, 'Severity_Score': -0.40009638446371837, 'Radius_meters': 200}\n",
      "{'Cluster': 17, 'Latitude': 40.694485846042134, 'Longitude': -73.96636244200344, 'Severity_Score': -0.4000963844637262, 'Radius_meters': 200}\n",
      "{'Cluster': 18, 'Latitude': 40.727556690039066, 'Longitude': -73.90394057246094, 'Severity_Score': 6.707157117491959, 'Radius_meters': 200}\n",
      "{'Cluster': 19, 'Latitude': 40.592086339084815, 'Longitude': -74.13136861480618, 'Severity_Score': 0.7846184893321183, 'Radius_meters': 200}\n",
      "{'Cluster': 20, 'Latitude': 40.718926212162685, 'Longitude': -73.85533430324209, 'Severity_Score': 0.7846758294130803, 'Radius_meters': 200}\n",
      "{'Cluster': 21, 'Latitude': 40.657958803949626, 'Longitude': -73.96563305297653, 'Severity_Score': 3.1540388992489525, 'Radius_meters': 200}\n",
      "{'Cluster': 22, 'Latitude': 40.76247062903226, 'Longitude': -73.90001415483871, 'Severity_Score': 13.37498365561703, 'Radius_meters': 200}\n",
      "{'Cluster': 23, 'Latitude': 40.76971600114853, 'Longitude': -73.85242195549223, 'Severity_Score': 1.9689369701497563, 'Radius_meters': 200}\n",
      "{'Cluster': 24, 'Latitude': 40.69505948580737, 'Longitude': -73.77182773490912, 'Severity_Score': -0.40009638446372614, 'Radius_meters': 200}\n",
      "{'Cluster': 25, 'Latitude': 40.857051768664654, 'Longitude': -73.87503149332822, 'Severity_Score': -0.4000963844637265, 'Radius_meters': 200}\n",
      "{'Cluster': 26, 'Latitude': 40.72874160990991, 'Longitude': -73.91260127297298, 'Severity_Score': 9.081577394046214, 'Radius_meters': 200}\n",
      "{'Cluster': 27, 'Latitude': 40.845527553749626, 'Longitude': -73.89110501107069, 'Severity_Score': 0.7849735799624852, 'Radius_meters': 200}\n",
      "{'Cluster': 28, 'Latitude': 40.686393351054335, 'Longitude': -73.88168440587634, 'Severity_Score': -0.4000963844637246, 'Radius_meters': 200}\n",
      "{'Cluster': 29, 'Latitude': 40.64491271674094, 'Longitude': -73.94693068643133, 'Severity_Score': 0.7846542408329743, 'Radius_meters': 200}\n",
      "{'Cluster': 30, 'Latitude': 40.7491253696606, 'Longitude': -73.97493462631415, 'Severity_Score': -0.4000963844637269, 'Radius_meters': 200}\n",
      "{'Cluster': 31, 'Latitude': 40.617369493307486, 'Longitude': -73.98614861294809, 'Severity_Score': -0.4000963844637252, 'Radius_meters': 200}\n",
      "{'Cluster': 32, 'Latitude': 40.6790346, 'Longitude': -73.9385077, 'Severity_Score': 49.35067812922593, 'Radius_meters': 0.0}\n",
      "{'Cluster': 33, 'Latitude': 40.721119924832216, 'Longitude': -73.93464733993288, 'Severity_Score': 0.19217474069926707, 'Radius_meters': 200}\n",
      "{'Cluster': 34, 'Latitude': 40.74754261818182, 'Longitude': -73.92361974545454, 'Severity_Score': 20.706292803162206, 'Radius_meters': 200}\n"
     ]
    }
   ],
   "source": [
    "def cluster_severity_hotspots(df):\n",
    "    \"\"\"\n",
    "    This function implements clustering as follows:\n",
    "    - Calculates a severity score for each crash by summing all injury counts.\n",
    "    - Filters the data to include only crashes with injuries for meaningful clustering.\n",
    "    - Applies KMeans clustering to group crashes into geographic hotspots, adjusting the number of clusters for granularity.\n",
    "    - Calculates a radius for each cluster to visualize the extent of the hotspot.\n",
    "    \"\"\"\n",
    "    print(\"Starting clustering for crash severity hotspots...\")\n",
    "    # Calculate severity score\n",
    "    df['SEVERITY_SCORE'] = (df['NUMBER OF PERSONS INJURED'] + df['NUMBER OF PEDESTRIANS INJURED'] +\n",
    "                            df['NUMBER OF CYCLIST INJURED'] + df['NUMBER OF MOTORIST INJURED'])\n",
    "    \n",
    "    # Filter for meaningful clusters (exclude rows with zero severity)\n",
    "    severity_data = df[df['SEVERITY_SCORE'] > 0][['LATITUDE', 'LONGITUDE', 'SEVERITY_SCORE']]\n",
    "    \n",
    "    if severity_data.empty:\n",
    "        print(\"No crashes with injuries to cluster.\")\n",
    "        return df\n",
    "\n",
    "    # Normalize severity score for clustering\n",
    "    severity_data['SEVERITY_SCORE'] = (severity_data['SEVERITY_SCORE'] - severity_data['SEVERITY_SCORE'].mean()) / severity_data['SEVERITY_SCORE'].std()\n",
    "\n",
    "    # Use fixed number of clusters\n",
    "    n_clusters = 35  # Increased for smaller clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    print(f\"Fitting KMeans with {n_clusters} clusters...\")\n",
    "    severity_data['Cluster'] = kmeans.fit_predict(severity_data[['LATITUDE', 'LONGITUDE', 'SEVERITY_SCORE']])\n",
    "\n",
    "    print(\"Clustering completed. Cluster centers:\")\n",
    "    cluster_info = []\n",
    "    for i, center in enumerate(kmeans.cluster_centers_):\n",
    "        print(f\"Cluster {i}: Location ({center[0]:.4f}, {center[1]:.4f}), Severity Score: {center[2]:.2f}\")\n",
    "        cluster_info.append({\n",
    "            'Cluster': i,\n",
    "            'Latitude': center[0],\n",
    "            'Longitude': center[1],\n",
    "            'Severity_Score': center[2]\n",
    "        })\n",
    "\n",
    "    # Calculate cluster radius\n",
    "    print(\"Calculating cluster radii...\")\n",
    "    max_radius = 200  # Cap radius at 300 meters for visualization clarity\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = severity_data[severity_data['Cluster'] == i]\n",
    "        distances = cluster_points.apply(lambda row: geodesic((row['LATITUDE'], row['LONGITUDE']),\n",
    "                                                              (kmeans.cluster_centers_[i][0], kmeans.cluster_centers_[i][1])).meters, axis=1)\n",
    "        cluster_radius = min(distances.max(), max_radius) if not distances.empty else 0\n",
    "        cluster_info[i]['Radius_meters'] = cluster_radius\n",
    "        print(f\"Cluster {i} radius: {cluster_radius:.2f} meters\")\n",
    "\n",
    "    # Merge cluster labels back to the main dataframe\n",
    "    df = df.merge(severity_data[['LATITUDE', 'LONGITUDE', 'Cluster']], on=['LATITUDE', 'LONGITUDE'], how='left')\n",
    "    print(\"Cluster assignment completed.\")\n",
    "\n",
    "    return df, cluster_info\n",
    "\n",
    "clustered_data, cluster_details = cluster_severity_hotspots(cleaned_data)\n",
    "\n",
    "print(\"Clustered Data Sample:\")\n",
    "print(clustered_data[['LATITUDE', 'LONGITUDE', 'SEVERITY_SCORE', 'Cluster']].dropna().sample(5))\n",
    "\n",
    "print(\"Cluster Details:\")\n",
    "for cluster in cluster_details:\n",
    "    print(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c7f5d-465d-41b8-9471-40a7c29d70f8",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "## Step 4: Visualization\n",
    "Finally, the clusters are plotted on an interactive map. Each cluster is represented by a marker, color-coded based on its severity score. The map provides a clear visual summary of crash hotspots across NYC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf55c25-ee73-4d08-be85-34da600ff16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating map visualization...\n",
      "Map saved as clusters_map.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/rws7_cn114j6khnyqkn98kpm0000gn/T/ipykernel_61016/2716406700.py:15: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colormap = cm.get_cmap('RdYlGn_r')  # Green to red scaling\n"
     ]
    }
   ],
   "source": [
    "def plot_clusters_on_map(cluster_info):\n",
    "    \"\"\"\n",
    "    This function generates an interactive map:\n",
    "    - Plots each cluster's centroid on the map with a color indicating its severity score.\n",
    "    - Uses a color gradient from green (low severity) to red (high severity) for better visual understanding.\n",
    "    - Saves the generated map to an HTML file for sharing or offline viewing.\n",
    "    \"\"\"\n",
    "    print(\"Creating map visualization...\")\n",
    "    # Create a base map centered at an approximate location\n",
    "    m = folium.Map(location=[40.7223, -73.9188], zoom_start=12)\n",
    "\n",
    "    # Normalize severity scores for color mapping\n",
    "    severity_scores = [cluster['Severity_Score'] for cluster in cluster_info]\n",
    "    norm = Normalize(vmin=min(severity_scores), vmax=max(severity_scores))\n",
    "    colormap = cm.get_cmap('RdYlGn_r')  # Green to red scaling\n",
    "\n",
    "    # Plot each cluster\n",
    "    for cluster in cluster_info:\n",
    "        # Map severity score to a color\n",
    "        color = mcolors.to_hex(colormap(norm(cluster['Severity_Score'])))\n",
    "\n",
    "        # Add a marker for the cluster centroid\n",
    "        folium.Marker(\n",
    "            location=[cluster['Latitude'], cluster['Longitude']],\n",
    "            popup=f\"Cluster {cluster['Cluster']}\\nSeverity Score: {cluster['Severity_Score']:.2f}\\nRadius: {cluster['Radius_meters']:.2f} meters\",\n",
    "            icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n",
    "        ).add_to(m)\n",
    "\n",
    "        # Add a circle representing the cluster radius\n",
    "        folium.Circle(\n",
    "            location=[cluster['Latitude'], cluster['Longitude']],\n",
    "            radius=cluster['Radius_meters'],\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_opacity=0.4\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Save or display the map\n",
    "    m.save(\"clusters_map.html\")\n",
    "    print(\"Map saved as clusters_map.html\")\n",
    "\n",
    "plot_clusters_on_map(cluster_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde2c5f-32e4-4e16-9609-596df38431c8",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### 1. Data Cleaning and Preparation\n",
    "- The dataset was successfully cleaned to handle missing values, invalid coordinates, and duplicates. Out of an initial dataset of over 2 million entries, approximately 1.89 million entries remained for analysis.\n",
    "- Contributing factors and crash severities were standardized to improve the quality and reliability of the analysis.\n",
    "\n",
    "### 2. Classification\n",
    "- The Gradient Boosting Classifier was trained to predict whether a crash involved injuries, achieving an overall accuracy of **64%**.\n",
    "- The model effectively balanced precision and recall, particularly after applying oversampling techniques like SMOTE to address class imbalance.\n",
    "- Approximately 70% of crashes involving injuries were correctly identified, providing actionable insights for injury-prone crash scenarios.\n",
    "\n",
    "### 3. Clustering for Severity Hotspots\n",
    "- Using KMeans clustering, **35 clusters** were identified to represent crash hotspots.\n",
    "- These clusters, created based on geographic coordinates and severity scores, provide a detailed view of where the most critical crashes occur in NYC.\n",
    "- Clusters with high severity scores (e.g., scores above 20) indicate critical areas that require targeted traffic safety interventions.\n",
    "\n",
    "### 4. Visualization\n",
    "- An interactive map was generated, showing the clusters with color-coded markers representing severity scores. Green indicates low severity, while red highlights high severity areas.\n",
    "- Each cluster is defined by a 200-meter radius, allowing for a localized and actionable understanding of crash hotspots.\n",
    "- The visualization provides a clear summary of crash hotspots, enabling stakeholders to prioritize safety measures effectively.\n",
    "\n",
    "### 5. Future Improvements\n",
    "- Classification accuracy can be further improved by incorporating additional features, such as weather conditions, traffic density, or time of day.\n",
    "- Clustering can be refined by dynamically adjusting the number of clusters or using advanced methods to capture micro-hotspots within the city.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
